{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85338f1d",
   "metadata": {},
   "source": [
    "## Naive CUDA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050894cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matrix_gpu_naive.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrix_gpu_naive.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matmul(float *a, float *b, float *c, int n) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < n && col < n) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < n; k++) {\n",
    "            sum += a[row * n + k] * b[k * n + col];\n",
    "        }\n",
    "        c[row * n + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    int n = 512;\n",
    "    if (argc > 1) {\n",
    "        n = atoi(argv[1]);\n",
    "    }\n",
    "\n",
    "    printf(\"Matrix size: %d x %d\\n\", n, n);\n",
    "\n",
    "    size_t size = n * n * sizeof(float);\n",
    "    \n",
    "    float *a = (float *)malloc(size);\n",
    "    float *b = (float *)malloc(size);\n",
    "    float *c = (float *)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n * n; i++) {\n",
    "        a[i] = rand() % 100 / 100.0f;\n",
    "        b[i] = rand() % 100 / 100.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    \n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks((n + 15) / 16, (n + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    matmul<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "    cudaEventRecord(stop);\n",
    "    \n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "    \n",
    "    printf(\"Time: %f ms\\n\", milliseconds);\n",
    "    \n",
    "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    \n",
    "    free(a);\n",
    "    free(b);\n",
    "    free(c);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f601a6",
   "metadata": {},
   "source": [
    "## Running CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017d8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  1 05:58:36 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb287b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a00b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 matrix_gpu_naive.cu -o matrix_gpu_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abe4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: 512 x 512\n",
      "Time: 1.222784 ms\n",
      "Matrix size: 1024 x 1024\n",
      "Time: 9.235648 ms\n",
      "Matrix size: 2048 x 2048\n",
      "Time: 74.927299 ms\n",
      "Matrix size: 4096 x 4096\n",
      "Time: 398.439148 ms\n",
      "Matrix size: 8192 x 8192\n",
      "Time: 2651.644531 ms\n"
     ]
    }
   ],
   "source": [
    "!./matrix_gpu_naive 512\n",
    "!./matrix_gpu_naive 1024\n",
    "!./matrix_gpu_naive 2048\n",
    "!./matrix_gpu_naive 4096\n",
    "!./matrix_gpu_naive 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf5c47",
   "metadata": {},
   "source": [
    "## Optimizing CUDA Code - Tiled CUDA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5172ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matrix_gpu_tiled.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrix_gpu_tiled.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#define TILE 16\n",
    "\n",
    "__global__ void matmul_tiled(float *a, float *b, float *c, int n) {\n",
    "    __shared__ float tile_a[TILE][TILE];\n",
    "    __shared__ float tile_b[TILE][TILE];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    \n",
    "    int row = blockIdx.y * TILE + ty;\n",
    "    int col = blockIdx.x * TILE + tx;\n",
    "\n",
    "    float sum = 0.0f;\n",
    "\n",
    "    for (int m = 0; m < (n + TILE - 1) / TILE; m++) {\n",
    "        if (row < n && (m * TILE + tx) < n)\n",
    "            tile_a[ty][tx] = a[row * n + m * TILE + tx];\n",
    "        else\n",
    "            tile_a[ty][tx] = 0.0f;\n",
    "\n",
    "        if (col < n && (m * TILE + ty) < n)\n",
    "            tile_b[ty][tx] = b[(m * TILE + ty) * n + col];\n",
    "        else\n",
    "            tile_b[ty][tx] = 0.0f;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        for (int k = 0; k < TILE; k++)\n",
    "            sum += tile_a[ty][k] * tile_b[k][tx];\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (row < n && col < n)\n",
    "        c[row * n + col] = sum;\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    int n = 512;\n",
    "    if (argc > 1) {\n",
    "        n = atoi(argv[1]);\n",
    "    }\n",
    "\n",
    "    printf(\"Matrix size: %d x %d\\n\", n, n);\n",
    "\n",
    "    size_t size = n * n * sizeof(float);\n",
    "    \n",
    "    float *a = (float *)malloc(size);\n",
    "    float *b = (float *)malloc(size);\n",
    "    float *c = (float *)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n * n; i++) {\n",
    "        a[i] = rand() % 100 / 100.0f;\n",
    "        b[i] = rand() % 100 / 100.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    \n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 threads(16, 16);\n",
    "    dim3 blocks((n + 15) / 16, (n + 15) / 16);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    matmul_tiled<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "    cudaEventRecord(stop);\n",
    "    \n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "    \n",
    "    printf(\"Time: %f ms\\n\", milliseconds);\n",
    "    \n",
    "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    \n",
    "    free(a);\n",
    "    free(b);\n",
    "    free(c);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5339f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 matrix_gpu_tiled.cu -o matrix_gpu_tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97d37c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: 512 x 512\n",
      "Time: 0.322944 ms\n",
      "Matrix size: 1024 x 1024\n",
      "Time: 2.198464 ms\n",
      "Matrix size: 2048 x 2048\n",
      "Time: 46.329983 ms\n",
      "Matrix size: 4096 x 4096\n",
      "Time: 292.058533 ms\n",
      "Matrix size: 8192 x 8192\n",
      "Time: 1739.154175 ms\n"
     ]
    }
   ],
   "source": [
    "!./matrix_gpu_tiled 512\n",
    "!./matrix_gpu_tiled 1024\n",
    "!./matrix_gpu_tiled 2048\n",
    "!./matrix_gpu_tiled 4096\n",
    "!./matrix_gpu_tiled 8192\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c55054",
   "metadata": {},
   "source": [
    "## Using cuBLAS Library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77444d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matrix_gpu_cublas.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrix_gpu_cublas.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cublas_v2.h>\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    int N;\n",
    "    if (argc > 1) {\n",
    "        N = atoi(argv[1]);\n",
    "    } else {\n",
    "        N = 1024; // default size\n",
    "    }\n",
    "    \n",
    "    size_t size = N * N * sizeof(float);\n",
    "\n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        h_A[i] = rand() % 100 / 100.0f;\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        h_B[i] = rand() % 100 / 100.0f;\n",
    "    }\n",
    "\n",
    "    float *d_A;\n",
    "    float *d_B;\n",
    "    float *d_C;\n",
    "    \n",
    "    cudaMalloc((void**)&d_A, size);\n",
    "    cudaMalloc((void**)&d_B, size);\n",
    "    cudaMalloc((void**)&d_C, size);\n",
    "\n",
    "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    cublasHandle_t handle;\n",
    "    cublasCreate(&handle);\n",
    "\n",
    "    float alpha = 1.0f;\n",
    "    float beta  = 0.0f;\n",
    "\n",
    "    cudaEvent_t start;\n",
    "    cudaEvent_t stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start);\n",
    "    cublasSgemm(handle,\n",
    "                CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                N, N, N,\n",
    "                &alpha,\n",
    "                d_B, N,\n",
    "                d_A, N,\n",
    "                &beta,\n",
    "                d_C, N);\n",
    "\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"cuBLAS SGEMM time (N=%d): %f ms\\n\", N, ms);\n",
    "\n",
    "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);   \n",
    "    volatile float sink = h_C[0];\n",
    "\n",
    "    cublasDestroy(handle);\n",
    "    \n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c6bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc matrix_gpu_cublas.cu -o matrix_gpu_cublas -lcublas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9f434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuBLAS SGEMM time (N=512): 5.489536 ms\n",
      "cuBLAS SGEMM time (N=1024): 6.142016 ms\n",
      "cuBLAS SGEMM time (N=2048): 11.465792 ms\n",
      "cuBLAS SGEMM time (N=4096): 53.734783 ms\n",
      "cuBLAS SGEMM time (N=8192): 293.303375 ms\n"
     ]
    }
   ],
   "source": [
    "!./matrix_gpu_cublas 512\n",
    "!./matrix_gpu_cublas 1024\n",
    "!./matrix_gpu_cublas 2048\n",
    "!./matrix_gpu_cublas 4096\n",
    "!./matrix_gpu_cublas 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bedabf0",
   "metadata": {},
   "source": [
    "## Creating a Shared Library and Using it in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4234b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matrix_lib.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matrix_lib.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "#define TILE_WIDTH 16\n",
    "\n",
    "__global__ void matrixMultiplyTiled(float *A, float *B, float *C, int N) {\n",
    "    __shared__ float ds_A[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float ds_B[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    \n",
    "    int Row = blockIdx.y * TILE_WIDTH + ty;\n",
    "    int Col = blockIdx.x * TILE_WIDTH + tx;\n",
    "\n",
    "    float Pvalue = 0.0f;\n",
    "\n",
    "    int numTiles = (N + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "    for (int m = 0; m < numTiles; ++m) {\n",
    "        int aCol = m * TILE_WIDTH + tx;\n",
    "        if (Row < N && aCol < N) {\n",
    "            ds_A[ty][tx] = A[Row * N + aCol];\n",
    "        } else {\n",
    "            ds_A[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        int bRow = m * TILE_WIDTH + ty;\n",
    "        if (Col < N && bRow < N) {\n",
    "            ds_B[ty][tx] = B[bRow * N + Col];\n",
    "        } else {\n",
    "            ds_B[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        for (int k = 0; k < TILE_WIDTH; ++k) {\n",
    "            Pvalue += ds_A[ty][k] * ds_B[k][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (Row < N && Col < N) {\n",
    "        C[Row * N + Col] = Pvalue;\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\" void gpu_matrix_multiply(float *h_A, float *h_B, float *h_C, int N) {\n",
    "    size_t size = N * N * sizeof(float);\n",
    "\n",
    "    float *d_A;\n",
    "    float *d_B;\n",
    "    float *d_C;\n",
    "    \n",
    "    cudaMalloc((void**)&d_A, size);\n",
    "    cudaMalloc((void**)&d_B, size);\n",
    "    cudaMalloc((void**)&d_C, size);\n",
    "\n",
    "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 block(TILE_WIDTH, TILE_WIDTH);\n",
    "    \n",
    "    int numBlocksX = (N + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "    int numBlocksY = (N + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "    dim3 grid(numBlocksX, numBlocksY);\n",
    "\n",
    "    matrixMultiplyTiled<<<grid, block>>>(d_A, d_B, d_C, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be17d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -Xcompiler -fPIC -shared matrix_lib.cu -o libmatrix.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b80565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48ed672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with matrix size N = 512\n",
      "CUDA library time (N=512): 1.9896 milliseconds\n",
      "\n",
      "Testing with matrix size N = 1024\n",
      "CUDA library time (N=1024): 3.7229 milliseconds\n",
      "\n",
      "Testing with matrix size N = 2048\n",
      "CUDA library time (N=2048): 20.6728 milliseconds\n",
      "\n",
      "Testing with matrix size N = 4096\n",
      "CUDA library time (N=4096): 75.4676 milliseconds\n",
      "\n",
      "Testing with matrix size N = 8192\n",
      "CUDA library time (N=8192): 290.0529 milliseconds\n"
     ]
    }
   ],
   "source": [
    "lib = ctypes.cdll.LoadLibrary(\"./libmatrix.so\")\n",
    "lib.gpu_matrix_multiply.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float32, ndim=1, flags=\"C_CONTIGUOUS\"),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float32, ndim=1, flags=\"C_CONTIGUOUS\"),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float32, ndim=1, flags=\"C_CONTIGUOUS\"),\n",
    "    ctypes.c_int\n",
    "]\n",
    "\n",
    "N = 512\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nTesting with matrix size N = {N}\")\n",
    "    \n",
    "    A = np.random.rand(N, N).astype(np.float32)\n",
    "    B = np.random.rand(N, N).astype(np.float32)\n",
    "    C = np.zeros((N, N), dtype=np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    lib.gpu_matrix_multiply(A.ravel(), B.ravel(), C.ravel(), N)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    print(f\"CUDA library time (N={N}): {elapsed_time * 1000:.4f} milliseconds\")\n",
    "    \n",
    "    N = N * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5bccb0",
   "metadata": {},
   "source": [
    "## Adding Custom Functions to the Shared Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa705c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conv_gpu.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile conv_gpu.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void convolution2D_GPU(\n",
    "    unsigned char *image,\n",
    "    int *kernel,\n",
    "    int *output,\n",
    "    int width,\n",
    "    int height\n",
    ") {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "    if (x >= width || y >= height) return;\n",
    "\n",
    "    int sum = 0;\n",
    "    int k = 1;  // 3x3 kernel radius\n",
    "\n",
    "    for (int ky = -k; ky <= k; ky++) {\n",
    "        for (int kx = -k; kx <= k; kx++) {\n",
    "            int ix = x + kx;\n",
    "            int iy = y + ky;\n",
    "            if (ix >= 0 && ix < width && iy >= 0 && iy < height) {\n",
    "                int pixel = image[iy * width + ix];\n",
    "                int weight = kernel[(ky + k) * 3 + (kx + k)];\n",
    "                sum += pixel * weight;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int width = 512;\n",
    "    int height = 512;\n",
    "    size_t img_size = width * height * sizeof(unsigned char);\n",
    "    size_t out_size = width * height * sizeof(int);\n",
    "\n",
    "    unsigned char *h_img = (unsigned char*)malloc(img_size);\n",
    "    int *h_out = (int*)malloc(out_size);\n",
    "\n",
    "    int h_kernel[9] = {\n",
    "        -1, -1, -1,\n",
    "        -1,  8, -1,\n",
    "        -1, -1, -1\n",
    "    };\n",
    "\n",
    "    for (int i = 0; i < width * height; i++)\n",
    "        h_img[i] = rand() % 256;\n",
    "\n",
    "    unsigned char *d_img;\n",
    "    int *d_kernel, *d_out;\n",
    "\n",
    "    cudaMalloc(&d_img, img_size);\n",
    "    cudaMalloc(&d_kernel, 9 * sizeof(int));\n",
    "    cudaMalloc(&d_out, out_size);\n",
    "\n",
    "    cudaMemcpy(d_img, h_img, img_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_kernel, h_kernel, 9 * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 block(16, 16);\n",
    "    dim3 grid((width + 15) / 16, (height + 15) / 16);\n",
    "\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "\n",
    "    cudaEventRecord(start);\n",
    "    convolution2D_GPU<<<grid, block>>>(d_img, d_kernel, d_out, width, height);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "\n",
    "    float ms;\n",
    "    cudaEventElapsedTime(&ms, start, stop);\n",
    "    printf(\"CUDA convolution time: %f ms\\n\", ms);\n",
    "\n",
    "    cudaMemcpy(h_out, d_out, out_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_img);\n",
    "    cudaFree(d_kernel);\n",
    "    cudaFree(d_out);\n",
    "    free(h_img);\n",
    "    free(h_out);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c88f175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc conv_gpu.cu -o conv_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d989833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA convolution time: 7.186432 ms\n"
     ]
    }
   ],
   "source": [
    "!./conv_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32063245",
   "metadata": {},
   "source": [
    "#### Adding and Using the Convolution to your Custom Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fa9ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conv_lib.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile conv_lib.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void convolution2D_GPU(\n",
    "    unsigned char *image,\n",
    "    int *kernel,\n",
    "    int *output,\n",
    "    int width,\n",
    "    int height\n",
    ") {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "    if (x >= width || y >= height) return;\n",
    "\n",
    "    int sum = 0;\n",
    "    int k = 1; // 3x3 kernel radius\n",
    "\n",
    "    for (int ky = -k; ky <= k; ky++) {\n",
    "        for (int kx = -k; kx <= k; kx++) {\n",
    "            int ix = x + kx;\n",
    "            int iy = y + ky;\n",
    "\n",
    "            if (ix >= 0 && ix < width && iy >= 0 && iy < height) {\n",
    "                int pixel = image[iy * width + ix];\n",
    "                int weight = kernel[(ky + k) * 3 + (kx + k)];\n",
    "                sum += pixel * weight;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output[y * width + x] = sum;\n",
    "}\n",
    "\n",
    "extern \"C\" void gpu_convolution(\n",
    "    unsigned char *h_image,\n",
    "    int *h_kernel,\n",
    "    int *h_output,\n",
    "    int width,\n",
    "    int height\n",
    ") {\n",
    "    size_t img_size = width * height * sizeof(unsigned char);\n",
    "    size_t out_size = width * height * sizeof(int);\n",
    "\n",
    "    unsigned char *d_image;\n",
    "    int *d_kernel;\n",
    "    int *d_output;\n",
    "\n",
    "    cudaMalloc((void**)&d_image, img_size);\n",
    "    cudaMalloc((void**)&d_kernel, 9 * sizeof(int));\n",
    "    cudaMalloc((void**)&d_output, out_size);\n",
    "\n",
    "    cudaMemcpy(d_image, h_image, img_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_kernel, h_kernel, 9 * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 block(16, 16);\n",
    "    dim3 grid((width + 15) / 16, (height + 15) / 16);\n",
    "\n",
    "    convolution2D_GPU<<<grid, block>>>(d_image, d_kernel, d_output, width, height);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, out_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_image);\n",
    "    cudaFree(d_kernel);\n",
    "    cudaFree(d_output);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37615c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -Xcompiler -fPIC -shared matrix_lib.cu conv_lib.cu -o libmatrix.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03f1861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000b0e5 T gpu_convolution\n"
     ]
    }
   ],
   "source": [
    "!nm -D libmatrix.so | grep gpu_convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cf69b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with image size: 512 x 512\n",
      "CUDA convolution time (512x512): 1.4246 milliseconds\n",
      "\n",
      "Testing with image size: 1024 x 1024\n",
      "CUDA convolution time (1024x1024): 3.8025 milliseconds\n",
      "\n",
      "Testing with image size: 2048 x 2048\n",
      "CUDA convolution time (2048x2048): 13.1280 milliseconds\n",
      "\n",
      "Testing with image size: 4096 x 4096\n",
      "CUDA convolution time (4096x4096): 51.6033 milliseconds\n",
      "\n",
      "Testing with image size: 8192 x 8192\n",
      "CUDA convolution time (8192x8192): 190.6698 milliseconds\n"
     ]
    }
   ],
   "source": [
    "if './libmatrix.so' in sys.modules:\n",
    "    del sys.modules['./libmatrix.so']\n",
    "\n",
    "lib_path = os.path.abspath('./libmatrix.so')\n",
    "lib = ctypes.CDLL(lib_path)\n",
    "\n",
    "gpu_conv_func = lib.gpu_convolution\n",
    "gpu_conv_func.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.uint8, ndim=1, flags=\"C_CONTIGUOUS\"),\n",
    "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags=\"C_CONTIGUOUS\"),\n",
    "    np.ctypeslib.ndpointer(dtype=np.int32, ndim=1, flags=\"C_CONTIGUOUS\"),\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int\n",
    "]\n",
    "\n",
    "width = 512\n",
    "height = 512\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nTesting with image size: {width} x {height}\")\n",
    "    \n",
    "    image = np.random.randint(0, 256, size=(height, width), dtype=np.uint8)\n",
    "    \n",
    "    kernel = np.array([\n",
    "        -1, -1, -1,\n",
    "        -1,  8, -1,\n",
    "        -1, -1, -1\n",
    "    ], dtype=np.int32)\n",
    "    \n",
    "    output = np.zeros((height, width), dtype=np.int32)\n",
    "    \n",
    "    start = time.time()\n",
    "    gpu_conv_func(\n",
    "        image.ravel(),\n",
    "        kernel,\n",
    "        output.ravel(),\n",
    "        width,\n",
    "        height\n",
    "    )\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    print(f\"CUDA convolution time ({width}x{height}): {elapsed_time * 1000:.4f} milliseconds\")\n",
    "    \n",
    "    width = width * 2\n",
    "    height = height * 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
